name: Remote Kubernetes Net Test

on:
  push:
    branches: [ 'devnet_*', 'testnet_*' ]
  pull_request:
  merge_group:
  workflow_dispatch:

# This allows a subsequently queued workflow run to interrupt previous runs on pull requests
concurrency:
  group: '${{ github.workflow }} @ ${{ github.event.pull_request.head.label || github.head_ref || github.run_id }}'
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  CARGO_INCREMENTAL: 0
  CARGO_NET_RETRY: 10
  RUST_BACKTRACE: full
  # We allow redundant explicit links because `cargo rdme` doesn't know how to resolve implicit intra-crate links.
  RUSTDOCFLAGS: -A rustdoc::redundant_explicit_links -D warnings
  RUSTFLAGS: -D warnings
  RUSTUP_MAX_RETRIES: 10
  RUST_LOG: linera=debug
  RUST_LOG_FORMAT: plain
  LINERA_STORAGE_SERVICE: 127.0.0.1:1235
  LINERA_WALLET: /tmp/local-linera-net/wallet_0.json
  LINERA_KEYSTORE: /tmp/local-linera-net/keystore_0.json
  LINERA_STORAGE: rocksdb:/tmp/local-linera-net/client_0.db
  LINERA_FAUCET_URL: http://localhost:8079
  LINERA_HELMFILE_TIMEOUT: 1800

permissions:
  contents: read

jobs:
  remote-kubernetes-net-test:
    if: ${{ github.event_name == 'merge_group' || github.event_name == 'workflow_dispatch' }}
    runs-on: linera-io-self-hosted-ci
    timeout-minutes: 90

    steps:
    - uses: actions/checkout@v4
    - uses: actions-rust-lang/setup-rust-toolchain@v1
    - name: Install Protoc
      uses: arduino/setup-protoc@v3
      with:
        repo-token: ${{ secrets.GITHUB_TOKEN }}
    - name: Install helmfile
      run: |
        # Install specific version to match local development environment
        HELMFILE_VERSION="v0.168.0"
        curl -fsSL -o helmfile.tar.gz "https://github.com/helmfile/helmfile/releases/download/${HELMFILE_VERSION}/helmfile_${HELMFILE_VERSION#v}_linux_amd64.tar.gz"
        tar -xzf helmfile.tar.gz helmfile
        sudo mv helmfile /usr/local/bin/
        rm helmfile.tar.gz
        # Verify installation
        which helmfile && helmfile --version
    - name: Build binaries
      run: |
        cargo build --features kubernetes,storage-service,opentelemetry --bin linera-server --bin linera-proxy --bin linera
    - name: Start validators with Kubernetes
      run: |
        mkdir /tmp/local-linera-net
        LOG_FILE="/tmp/linera-net-up-kubernetes.log"
        cargo run --bin linera --features kubernetes,storage-service,opentelemetry -- net up --kubernetes \
          --policy-config testnet --path /tmp/local-linera-net --validators 1 --shards 2 \
          > "$LOG_FILE" 2>&1 &
        NETWORK_PID=$!

        # Stream logs in background so they appear in CI output
        tail -f "$LOG_FILE" 2>/dev/null &
        TAIL_PID=$!

        # Background diagnostics: periodically dump pod status once a Kind cluster exists.
        (
          KIND_CTX=""
          while [ -z "$KIND_CTX" ]; do
            KIND_CTX=$(kubectl config get-contexts -o name 2>/dev/null | grep '^kind-' | head -1)
            sleep 5
          done
          echo "=== DIAGNOSTICS: Found Kind context: $KIND_CTX ==="
          while kill -0 "$NETWORK_PID" 2>/dev/null && ! grep -q "READY!" "$LOG_FILE" 2>/dev/null; do
            echo "=== DIAGNOSTICS $(date -u +%H:%M:%S): Pod status ==="
            kubectl --context "$KIND_CTX" get pods --all-namespaces -o wide 2>&1 || true
            echo "=== DIAGNOSTICS $(date -u +%H:%M:%S): Events (warnings) ==="
            kubectl --context "$KIND_CTX" get events --all-namespaces --field-selector type=Warning --sort-by='.lastTimestamp' 2>&1 | tail -20 || true
            echo "=== END DIAGNOSTICS ==="
            sleep 30
          done
        ) &
        DIAG_PID=$!

        # Wait for "READY!" message (includes Docker image build + helm sync + port-forwards).
        # No explicit timeout here; the job-level timeout-minutes: 90 is the safeguard.
        echo "Waiting for validators to deploy (PID: $NETWORK_PID)..."
        while ! grep -q "READY!" "$LOG_FILE" 2>/dev/null; do
          if ! kill -0 "$NETWORK_PID" 2>/dev/null; then
            echo "ERROR: Network process (PID: $NETWORK_PID) died"
            echo "=== FINAL DIAGNOSTICS: Pod status at failure ==="
            KIND_CTX=$(kubectl config get-contexts -o name 2>/dev/null | grep '^kind-' | head -1)
            if [ -n "$KIND_CTX" ]; then
              kubectl --context "$KIND_CTX" get pods --all-namespaces -o wide 2>&1 || true

              echo "=== Container status for non-fully-Ready pods ==="
              kubectl --context "$KIND_CTX" get pods --all-namespaces -o json 2>/dev/null \
                | python3 -c "
        import json, sys
        data = json.load(sys.stdin)
        for item in data.get('items', []):
            name = item['metadata']['name']
            ns = item['metadata']['namespace']
            all_statuses = item.get('status', {}).get('containerStatuses', []) + item.get('status', {}).get('initContainerStatuses', [])
            not_ready = [cs for cs in all_statuses if not cs.get('ready', False)]
            if not_ready:
                print(f'--- {ns}/{name} ---')
                for cs in all_statuses:
                    state = cs.get('state', {})
                    ready = cs.get('ready', False)
                    restarts = cs.get('restartCount', 0)
                    marker = ' (NOT READY)' if not ready else ''
                    for k, v in state.items():
                        print(f'  {cs[\"name\"]}: {k} restarts={restarts}{marker} - {v}')
        " 2>&1 || true

              echo "=== ScyllaDB pod logs (scylla container) ==="
              SCYLLA_POD=$(kubectl --context "$KIND_CTX" -n scylla get pods -l "app.kubernetes.io/name=scylla" -o name 2>/dev/null | head -1)
              if [ -n "$SCYLLA_POD" ]; then
                echo "--- scylla container (last 80 lines) ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c scylla --tail=80 2>&1 || true
                echo "--- sysctl-buddy init container ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c sysctl-buddy 2>&1 || true
                echo "--- scylladb-api-status-probe sidecar (last 40 lines) ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c scylladb-api-status-probe --tail=40 2>&1 || true
                echo "--- scylladb-ignition sidecar (last 40 lines) ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c scylladb-ignition --tail=40 2>&1 || true
                echo "--- scylla-manager-agent sidecar (last 40 lines) ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c scylla-manager-agent --tail=40 2>&1 || true
                echo "--- scylla pod describe (conditions + events) ---"
                kubectl --context "$KIND_CTX" -n scylla describe "$SCYLLA_POD" 2>&1 | grep -A 100 "^Conditions:" || true
              fi

              echo "=== Promtail logs (last 40 lines) ==="
              PROMTAIL_POD=$(kubectl --context "$KIND_CTX" -n default get pods -l "app.kubernetes.io/name=promtail" -o name 2>/dev/null | head -1)
              if [ -n "$PROMTAIL_POD" ]; then
                kubectl --context "$KIND_CTX" -n default logs "$PROMTAIL_POD" --tail=40 2>&1 || true
              fi

              echo "=== Proxy init container logs ==="
              kubectl --context "$KIND_CTX" -n default logs proxy-0 -c linera-proxy-initializer --tail=20 2>&1 || true

              echo "=== Node resources ==="
              kubectl --context "$KIND_CTX" top nodes 2>&1 || true
              kubectl --context "$KIND_CTX" describe nodes 2>&1 | grep -A 20 "Allocated resources:" || true

              echo "=== Host kernel params (aio-max-nr) ==="
              kubectl --context "$KIND_CTX" -n scylla exec "$SCYLLA_POD" -c scylla -- cat /proc/sys/fs/aio-max-nr 2>&1 || true
              kubectl --context "$KIND_CTX" -n scylla exec "$SCYLLA_POD" -c scylla -- cat /proc/sys/fs/aio-nr 2>&1 || true

              echo "=== Recent events ==="
              kubectl --context "$KIND_CTX" get events --all-namespaces --sort-by='.lastTimestamp' 2>&1 | tail -40 || true
            fi
            cat "$LOG_FILE"
            exit 1
          fi
          sleep 2
        done
        echo "Validators reported READY"

        kill $DIAG_PID 2>/dev/null || true

        # Wait for proxy to accept TCP connections through port-forward.
        # No explicit timeout; the job-level timeout-minutes: 90 is the safeguard.
        echo "Waiting for proxy at 127.0.0.1:19100..."
        while ! (echo > /dev/tcp/127.0.0.1/19100) 2>/dev/null; do
          sleep 2
        done
        echo "Proxy is accepting connections"

        kill $TAIL_PID 2>/dev/null || true
    - name: Start faucet and wait for it
      run: |
        FAUCET_PORT=$(echo "$LINERA_FAUCET_URL" | cut -d: -f3)
        FAUCET_LOG="/tmp/linera-faucet.log"
        cargo run --bin linera --features kubernetes,storage-service,opentelemetry -- faucet \
          --port "$FAUCET_PORT" --amount 1000 --storage-path /tmp/faucet.sqlite \
          > "$FAUCET_LOG" 2>&1 &
        FAUCET_PID=$!
        echo "Started faucet (PID: $FAUCET_PID)"
        bash scripts/wait-for-kubernetes-service.sh "$LINERA_FAUCET_URL" "$FAUCET_PID" "$FAUCET_LOG"
    - name: Run the Kubernetes tests
      run: |
        cargo test -p linera-service remote_net_grpc --features remote-net,opentelemetry
