name: Remote Kubernetes Net Test

on:
  push:
    branches: [ 'devnet_*', 'testnet_*' ]
  pull_request:
  merge_group:
  workflow_dispatch:

# This allows a subsequently queued workflow run to interrupt previous runs on pull requests
concurrency:
  group: '${{ github.workflow }} @ ${{ github.event.pull_request.head.label || github.head_ref || github.run_id }}'
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  CARGO_INCREMENTAL: 0
  CARGO_NET_RETRY: 10
  RUST_BACKTRACE: full
  # We allow redundant explicit links because `cargo rdme` doesn't know how to resolve implicit intra-crate links.
  RUSTDOCFLAGS: -A rustdoc::redundant_explicit_links -D warnings
  RUSTFLAGS: -D warnings
  RUSTUP_MAX_RETRIES: 10
  RUST_LOG: linera=debug
  RUST_LOG_FORMAT: plain
  LINERA_STORAGE_SERVICE: 127.0.0.1:1235
  LINERA_WALLET: /tmp/local-linera-net/wallet_0.json
  LINERA_KEYSTORE: /tmp/local-linera-net/keystore_0.json
  LINERA_STORAGE: rocksdb:/tmp/local-linera-net/client_0.db
  LINERA_FAUCET_URL: http://localhost:8079
  LINERA_HELMFILE_TIMEOUT: 1800
  LINERA_HELMFILE_SET_PROXY_SEND_TIMEOUT: 120000
  LINERA_HELMFILE_SET_PROXY_RECV_TIMEOUT: 120000
  LINERA_HELMFILE_SET_SCYLLA_CPU: 1
  LINERA_HELMFILE_SET_SCYLLA_MEMORY: 2Gi
  LINERA_HELMFILE_SET_LOKI_ENABLED: "false"
  LINERA_HELMFILE_SET_KUBE_PROMETHEUS_ENABLED: "false"
  KUBECONFIG: /tmp/kind-kubeconfig

permissions:
  contents: read

jobs:
  remote-kubernetes-net-test:
    if: ${{ github.event_name == 'merge_group' || github.event_name == 'workflow_dispatch' }}
    runs-on: linera-io-self-hosted-ci
    timeout-minutes: 90

    steps:
    - uses: actions/checkout@v4
    - uses: actions-rust-lang/setup-rust-toolchain@v1
    - name: Install Protoc
      uses: arduino/setup-protoc@v3
      with:
        repo-token: ${{ secrets.GITHUB_TOKEN }}
    - name: Install helmfile
      run: |
        # Install specific version to match local development environment
        HELMFILE_VERSION="v0.168.0"
        curl -fsSL -o helmfile.tar.gz "https://github.com/helmfile/helmfile/releases/download/${HELMFILE_VERSION}/helmfile_${HELMFILE_VERSION#v}_linux_amd64.tar.gz"
        tar -xzf helmfile.tar.gz helmfile
        sudo mv helmfile /usr/local/bin/
        rm helmfile.tar.gz
        # Verify installation
        which helmfile && helmfile --version
    - name: Build binaries
      run: |
        cargo build --features kubernetes,storage-service,opentelemetry --bin linera-server --bin linera-proxy --bin linera
    - name: Start validators with Kubernetes
      run: |
        mkdir /tmp/local-linera-net
        LOG_FILE="/tmp/linera-net-up-kubernetes.log"
        cargo run --bin linera --features kubernetes,storage-service,opentelemetry -- net up --kubernetes \
          --policy-config testnet --path /tmp/local-linera-net --validators 1 --shards 2 \
          > "$LOG_FILE" 2>&1 &
        NETWORK_PID=$!

        # Stream logs in background so they appear in CI output
        tail -f "$LOG_FILE" 2>/dev/null &
        TAIL_PID=$!

        # Fix Kind context namespace: the ARC runner pod's in-cluster namespace
        # is "arc-runners", which kubectl uses as default when the context doesn't
        # specify one. Set it to "default" as soon as the Kind context appears.
        (
          while true; do
            KIND_CTX=$(kubectl config get-contexts -o name 2>/dev/null | grep '^kind-' | head -1)
            if [ -n "$KIND_CTX" ]; then
              kubectl config set-context "$KIND_CTX" --namespace=default
              echo "Fixed namespace for context $KIND_CTX to 'default'"
              break
            fi
            sleep 1
          done
        ) &

        # Background diagnostics: periodically dump pod status once a Kind cluster exists.
        (
          KIND_CTX=""
          while [ -z "$KIND_CTX" ]; do
            KIND_CTX=$(kubectl config get-contexts -o name 2>/dev/null | grep '^kind-' | head -1)
            sleep 5
          done
          echo "=== DIAGNOSTICS: Found Kind context: $KIND_CTX ==="
          while kill -0 "$NETWORK_PID" 2>/dev/null && ! grep -q "READY!" "$LOG_FILE" 2>/dev/null; do
            echo "=== DIAGNOSTICS $(date -u +%H:%M:%S): Pod status ==="
            kubectl --context "$KIND_CTX" get pods --all-namespaces -o wide 2>&1 || true
            echo "=== DIAGNOSTICS $(date -u +%H:%M:%S): Events (warnings) ==="
            kubectl --context "$KIND_CTX" get events --all-namespaces --field-selector type=Warning --sort-by='.lastTimestamp' 2>&1 | tail -20 || true
            echo "=== END DIAGNOSTICS ==="
            sleep 30
          done
        ) &
        DIAG_PID=$!

        # Wait for the exact READY message from `linera net up`.
        # No explicit timeout here; the job-level timeout-minutes: 90 is the safeguard.
        echo "Waiting for validators to deploy (PID: $NETWORK_PID)..."
        while ! grep -qF "READY!" "$LOG_FILE" 2>/dev/null; do
          if ! kill -0 "$NETWORK_PID" 2>/dev/null; then
            echo "ERROR: Network process (PID: $NETWORK_PID) died"
            echo "=== FINAL DIAGNOSTICS: Pod status at failure ==="
            KIND_CTX=$(kubectl config get-contexts -o name 2>/dev/null | grep '^kind-' | head -1)
            if [ -n "$KIND_CTX" ]; then
              kubectl --context "$KIND_CTX" get pods --all-namespaces -o wide 2>&1 || true

              echo "=== Container status for non-fully-Ready pods ==="
              kubectl --context "$KIND_CTX" get pods --all-namespaces -o json 2>/dev/null \
                | python3 -c "
        import json, sys
        data = json.load(sys.stdin)
        for item in data.get('items', []):
            name = item['metadata']['name']
            ns = item['metadata']['namespace']
            all_statuses = item.get('status', {}).get('containerStatuses', []) + item.get('status', {}).get('initContainerStatuses', [])
            not_ready = [cs for cs in all_statuses if not cs.get('ready', False)]
            if not_ready:
                print(f'--- {ns}/{name} ---')
                for cs in all_statuses:
                    state = cs.get('state', {})
                    ready = cs.get('ready', False)
                    restarts = cs.get('restartCount', 0)
                    marker = ' (NOT READY)' if not ready else ''
                    for k, v in state.items():
                        print(f'  {cs[\"name\"]}: {k} restarts={restarts}{marker} - {v}')
        " 2>&1 || true

              echo "=== ScyllaDB pod logs (scylla container) ==="
              SCYLLA_POD=$(kubectl --context "$KIND_CTX" -n scylla get pods -l "app.kubernetes.io/name=scylla" -o name 2>/dev/null | head -1)
              if [ -n "$SCYLLA_POD" ]; then
                echo "--- scylla container (last 80 lines) ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c scylla --tail=80 2>&1 || true
                echo "--- sysctl-buddy init container ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c sysctl-buddy 2>&1 || true
                echo "--- scylladb-api-status-probe sidecar (last 40 lines) ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c scylladb-api-status-probe --tail=40 2>&1 || true
                echo "--- scylladb-ignition sidecar (last 40 lines) ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c scylladb-ignition --tail=40 2>&1 || true
                echo "--- scylla-manager-agent sidecar (last 40 lines) ---"
                kubectl --context "$KIND_CTX" -n scylla logs "$SCYLLA_POD" -c scylla-manager-agent --tail=40 2>&1 || true
                echo "--- scylla pod describe (conditions + events) ---"
                kubectl --context "$KIND_CTX" -n scylla describe "$SCYLLA_POD" 2>&1 | grep -A 100 "^Conditions:" || true
              fi

              echo "=== Promtail logs (last 40 lines) ==="
              PROMTAIL_POD=$(kubectl --context "$KIND_CTX" -n default get pods -l "app.kubernetes.io/name=promtail" -o name 2>/dev/null | head -1)
              if [ -n "$PROMTAIL_POD" ]; then
                kubectl --context "$KIND_CTX" -n default logs "$PROMTAIL_POD" --tail=40 2>&1 || true
              fi

              echo "=== Proxy init container logs ==="
              kubectl --context "$KIND_CTX" -n default logs proxy-0 -c linera-proxy-initializer --tail=20 2>&1 || true

              echo "=== Node resources ==="
              kubectl --context "$KIND_CTX" top nodes 2>&1 || true
              kubectl --context "$KIND_CTX" describe nodes 2>&1 | grep -A 20 "Allocated resources:" || true

              echo "=== Host kernel params (aio-max-nr) ==="
              kubectl --context "$KIND_CTX" -n scylla exec "$SCYLLA_POD" -c scylla -- cat /proc/sys/fs/aio-max-nr 2>&1 || true
              kubectl --context "$KIND_CTX" -n scylla exec "$SCYLLA_POD" -c scylla -- cat /proc/sys/fs/aio-nr 2>&1 || true

              echo "=== Recent events ==="
              kubectl --context "$KIND_CTX" get events --all-namespaces --sort-by='.lastTimestamp' 2>&1 | tail -40 || true
            fi
            cat "$LOG_FILE"
            exit 1
          fi
          sleep 2
        done
        echo "Validators reported READY"

        kill $DIAG_PID 2>/dev/null || true
        kill $TAIL_PID 2>/dev/null || true

        # Verify that proxy and shard pods are actually Running (not stuck in Init).
        KIND_CTX=$(kubectl config get-contexts -o name 2>/dev/null | grep '^kind-' | head -1)
        echo "=== Post-READY pod status ==="
        kubectl --context "$KIND_CTX" get pods -o wide
        NOT_READY=$(kubectl --context "$KIND_CTX" get pods --no-headers 2>/dev/null \
          | grep -cEv 'Running|Completed' || true)
        if [ "$NOT_READY" -gt 0 ]; then
          echo "WARNING: $NOT_READY pod(s) not Running after READY. Waiting up to 120s..."
          kubectl --context "$KIND_CTX" wait --for=condition=Ready pod --all --timeout=120s || {
            echo "ERROR: pods did not become Ready"
            kubectl --context "$KIND_CTX" get pods -o wide
            kubectl --context "$KIND_CTX" describe pods | grep -A 5 "State:\|Reason:" || true
            exit 1
          }
          echo "All pods now Ready"
          kubectl --context "$KIND_CTX" get pods -o wide
        fi

        echo "=== Proxy pod command and env (diagnostic) ==="
        kubectl --context "$KIND_CTX" get pod proxy-0 -o jsonpath='{.spec.containers[?(@.name=="linera-proxy")].command}' 2>&1 || true
        echo ""
        kubectl --context "$KIND_CTX" get pod proxy-0 -o jsonpath='{.spec.containers[?(@.name=="linera-proxy")].env}' 2>&1 || true
        echo ""
    - name: Start faucet and wait for it
      run: |
        FAUCET_PORT=$(echo "$LINERA_FAUCET_URL" | cut -d: -f3)
        FAUCET_LOG="/tmp/linera-faucet.log"
        cargo run --bin linera --features kubernetes,storage-service,opentelemetry -- faucet \
          --port "$FAUCET_PORT" --amount 1000 --storage-path /tmp/faucet.sqlite \
          > "$FAUCET_LOG" 2>&1 &
        FAUCET_PID=$!
        echo "Started faucet (PID: $FAUCET_PID)"
        bash scripts/wait-for-kubernetes-service.sh "$LINERA_FAUCET_URL" "$FAUCET_PID" "$FAUCET_LOG"
    - name: Run the Kubernetes tests
      run: |
        cargo test -p linera-service remote_net_grpc --features remote-net,opentelemetry
    - name: Post-test diagnostics
      if: always()
      run: |
        KIND_CTX=$(kubectl config get-contexts -o name 2>/dev/null | grep '^kind-' | head -1)
        if [ -n "$KIND_CTX" ]; then
          echo "=== Pod status (restart counts) ==="
          kubectl --context "$KIND_CTX" get pods --all-namespaces -o wide
          echo "=== OOMKilled / CrashLoopBackOff events ==="
          kubectl --context "$KIND_CTX" get events --all-namespaces --field-selector reason=OOMKilling --sort-by='.lastTimestamp' 2>&1 || true
          kubectl --context "$KIND_CTX" get events --all-namespaces --field-selector reason=OOMKilled --sort-by='.lastTimestamp' 2>&1 || true
          echo "=== Container last state (shows OOM exits) ==="
          kubectl --context "$KIND_CTX" get pods -o json 2>/dev/null \
            | python3 -c "
        import json, sys
        data = json.load(sys.stdin)
        for item in data.get('items', []):
            name = item['metadata']['name']
            for cs in item.get('status', {}).get('containerStatuses', []):
                restarts = cs.get('restartCount', 0)
                last = cs.get('lastState', {})
                if restarts > 0 or last:
                    print(f'{name}/{cs[\"name\"]}: restarts={restarts} lastState={last}')
        " 2>&1 || true
          echo "=== Recent warning/error events ==="
          kubectl --context "$KIND_CTX" get events --all-namespaces --sort-by='.lastTimestamp' 2>&1 | tail -30 || true
          echo "=== Node memory pressure ==="
          kubectl --context "$KIND_CTX" describe nodes 2>&1 | grep -A 5 "MemoryPressure\|Allocatable:" || true

          echo "=== Proxy logs (errors/panics/timeouts) ==="
          kubectl --context "$KIND_CTX" logs proxy-0 -c linera-proxy 2>&1 | grep -iE "error|panic|timeout|cancel|reset|CANCEL|pending_blob" | tail -80 || true
          echo "=== Proxy logs (last 50 lines) ==="
          kubectl --context "$KIND_CTX" logs proxy-0 -c linera-proxy --tail=50 2>&1 || true

          echo "=== Shard-0 logs (errors/panics/timeouts) ==="
          kubectl --context "$KIND_CTX" logs shards-0 -c linera-server 2>&1 | grep -iE "error|panic|timeout|cancel|reset|pending_blob" | tail -80 || true
          echo "=== Shard-0 logs (last 50 lines) ==="
          kubectl --context "$KIND_CTX" logs shards-0 -c linera-server --tail=50 2>&1 || true

          echo "=== Shard-1 logs (errors/panics/timeouts) ==="
          kubectl --context "$KIND_CTX" logs shards-1 -c linera-server 2>&1 | grep -iE "error|panic|timeout|cancel|reset|pending_blob" | tail -80 || true
          echo "=== Shard-1 logs (last 50 lines) ==="
          kubectl --context "$KIND_CTX" logs shards-1 -c linera-server --tail=50 2>&1 || true
        fi
