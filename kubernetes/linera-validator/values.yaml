# Default values for linera-validator chart
# This file provides sane defaults for all configuration options.
# Override these values using environment variables or custom values files.

# ============================================================================
# Core Linera Configuration
# ============================================================================

# Container image configuration
lineraImage: "linera:latest"
lineraImagePullPolicy: "IfNotPresent"

# Logging configuration
logLevel: "info"

# OpenTelemetry configuration
# Set to empty string to disable OTLP tracing
# When otelCollector.enabled is true, this should point to the collector
# Example: "http://otel-collector.default.svc.cluster.local:4317"
otlpExporterEndpoint: ""

# Network ports
proxyPort: 19100
shardPort: 19100
metricsPort: 21100

# Deployment scaling
numShards: 4
numProxies: 1

# Shard container resource limits
# These should be calculated based on node size and shard count
# to ensure cgroup-aware memory/CPU detection works correctly in RocksDB
shardResources: {}
# Example:
#   requests:
#     cpu: "2"
#     memory: "8Gi"
#   limits:
#     cpu: "4"
#     memory: "16Gi"

# Proxy container resource limits
proxyResources: {}
# Example:
#   requests:
#     cpu: "1"
#     memory: "4Gi"
#   limits:
#     cpu: "2"
#     memory: "8Gi"

# Performance tuning
# Set to empty string to use default Tokio thread count
serverTokioThreads: ""

# Cross-chain message queue size (default in binary is 1000)
# Set to empty string to use default
crossChainQueueSize: ""

# Notification queue size for proxy notifications (default in binary is 1000)
# Set to empty string to use default
notificationQueueSize: ""

# ============================================================================
# Storage Configuration
# ============================================================================

# Primary storage backend
# Default: ScyllaDB connection string
storage: "scylladb:tcp:scylla-client.scylla.svc.cluster.local:9042"

# Storage replication factor
storageReplicationFactor: 1

# Dual storage mode (RocksDB + ScyllaDB)
dualStore: false

# RocksDB storage size per shard (only used if dualStore is true)
rocksdbStorageSize: "2Gi"

# Local SSD usage flag (GCP-specific)
usingLocalSsd: false

# GCP-specific run flag
gcpRun: false

# ============================================================================
# Validator Configuration
# ============================================================================

validator:
  # Server configuration file path
  serverConfig: "working/server_1.json"
  # Genesis configuration file path
  genesisConfig: "working/genesis.json"

# ============================================================================
# Explorer Services (Indexer, Block Exporter, Web Explorer)
# ============================================================================

# Block Exporter - exports blocks for indexing
blockExporter:
  enabled: false
  image: "linera-exporter:latest"
  imagePullPolicy: "IfNotPresent"
  replicas: 1
  port: 8882
  metricsPort: 9091
  storageSize: "1Gi"
  logLevel: "info"
  serviceMonitor:
    enabled: false

# Indexer - indexes blockchain data
indexer:
  enabled: false
  image: "linera-indexer:latest"
  imagePullPolicy: "IfNotPresent"
  port: 8081
  databasePath: "/data/indexer.db"
  storageSize: "2Gi"
  logLevel: "info"

# Explorer - web interface for blockchain exploration
explorer:
  enabled: false
  image: "linera-explorer:latest"
  imagePullPolicy: "IfNotPresent"
  frontendPort: 3001
  apiPort: 3002
  logLevel: "info"
  ingress:
    enabled: false
    annotations: {}
    hosts:
      - host: "linera-explorer.local"
        paths:
          - path: "/"
            pathType: "Prefix"
    tls: []
    # Example TLS configuration:
    # tls:
    #   - secretName: explorer-tls
    #     hosts:
    #       - linera-explorer.example.com

# ============================================================================
# Environment Configuration
# ============================================================================

# Environment type: "kind" (local), "GCP", etc.
environment: "kind"

# GCP-specific configuration (only used when environment is "GCP")
staticIpGcpName: ""
validatorDomainName: ""

# ============================================================================
# Monitoring & Observability
# ============================================================================

# Grafana Cloud integration
writeToGrafanaCloud: false
grafanaCloudUsername: ""
grafanaCloudAPIToken: ""

# Validator label for external metrics systems
validatorLabel: "validator-1"

# ============================================================================
# Optional Monitoring Stack Dependencies
# ============================================================================
# These are disabled by default. Enable explicitly when needed.

# Alloy - centralized observability collector
# Enable via LINERA_HELMFILE_SET_ALLOY_ENABLED=true or by setting alloy.enabled=true
alloy:
  enabled: false
  # Alloy subchart configuration
  # When enabled, provide the following environment variables:
  # - ALLOY_CLUSTER_NAME: Name of the cluster (e.g., "prod-cluster-1")
  # - ALLOY_VALIDATOR_NAME: Name of the validator (e.g., "validator-1")
  # - ALLOY_PROMETHEUS_ENABLED: Enable Prometheus metrics export (true/false)
  # - ALLOY_PROMETHEUS_URL: OTLP endpoint for metrics (e.g., "https://prometheus-endpoint/otlp")
  # - ALLOY_PROMETHEUS_USER: Username for Prometheus authentication
  # - ALLOY_PROMETHEUS_PASS: Password for Prometheus authentication
  # - ALLOY_LOKI_ENABLED: Enable Loki logs export (true/false)
  # - ALLOY_LOKI_URL: Loki push endpoint for logs (e.g., "https://loki-endpoint/loki/api/v1/push")
  # - ALLOY_LOKI_USER: Username for Loki authentication
  # - ALLOY_LOKI_PASS: Password for Loki authentication
  # - ALLOY_TEMPO_ENABLED: Enable Tempo traces export (true/false)
  # - ALLOY_TEMPO_URL: OTLP endpoint for traces (e.g., "https://tempo-endpoint/otlp")
  # - ALLOY_TEMPO_USER: Username for Tempo authentication
  # - ALLOY_TEMPO_PASS: Password for Tempo authentication
  alloy:
    configMap:
      create: true
      # The alloy-config.river.tpl file contains the Alloy River configuration
      # It will be templated and mounted into the Alloy pods
      content: |-
        // NOTE: This is a placeholder. In production, replace with actual River config
        // or use values-local.yaml.gotmpl which loads from alloy-config.river.tpl file.
        // For reference config, see alloy-config.river.tpl in the chart directory.

        // Minimal working configuration - customize as needed
        prometheus.exporter.self "alloy" {}

        prometheus.scrape "alloy_metrics" {
          targets    = prometheus.exporter.self.alloy.targets
          forward_to = []
        }
    extraEnv:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: CLUSTER_NAME
        value: ""
      - name: VALIDATOR_NAME
        value: ""
      - name: PROMETHEUS_ENABLED
        value: "false"
      - name: PROMETHEUS_OTLP_URL
        value: ""
      - name: PROMETHEUS_OTLP_USER
        value: ""
      - name: PROMETHEUS_OTLP_PASS
        value: ""
      - name: LOKI_ENABLED
        value: "false"
      - name: LOKI_PUSH_URL
        value: ""
      - name: LOKI_PUSH_USER
        value: ""
      - name: LOKI_PUSH_PASS
        value: ""
      - name: TEMPO_ENABLED
        value: "false"
      - name: TEMPO_OTLP_URL
        value: ""
      - name: TEMPO_OTLP_USER
        value: ""
      - name: TEMPO_OTLP_PASS
        value: ""

# Loki Stack - log aggregation
loki-stack:
  enabled: false
  loki:
    enabled: false
    isDefault: false
    persistence:
      enabled: true
      size: "1Gi"
    config:
      limits_config:
        reject_old_samples_max_age: "24h"
  promtail:
    enabled: false
    config:
      clients:
        - url: "http://linera-core-loki:3100/loki/api/v1/push"

# Kube Prometheus Stack - metrics and alerting
kube-prometheus-stack:
  enabled: false
  grafana:
    sidecar:
      dashboards:
        enabled: true
        label: "grafana_dashboard"
        labelValue: "1"
        folderAnnotation: "grafana_folder"
        provider:
          foldersFromFilesStructure: true
    persistence:
      enabled: true
      size: "1Gi"
    plugins:
      - "grafana-piechart-panel"
  prometheus:
    prometheusSpec:
      scrapeInterval: "30s"
      retention: "2d"
      retentionSize: "1GB"
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: "standard"
            accessModes:
              - "ReadWriteOnce"
            resources:
              requests:
                storage: "1Gi"
      # Auto-discover ServiceMonitors across all namespaces
      serviceMonitorSelector: {}
      serviceMonitorNamespaceSelector: {}
      serviceMonitorSelectorNilUsesHelmValues: false
      ruleSelector: {}
      ruleSelectorNilUsesHelmValues: false

# Pyroscope - continuous profiling
pyroscope:
  enabled: false

# ============================================================================
# OpenTelemetry Collector - Two-Tier Architecture
# ============================================================================
# Tier 1 (Router): DaemonSet on shard nodes, routes by traceID
# Tier 2 (Sampler): StatefulSet on monitoring nodes, does tail sampling

otelCollector:
  # Enable/disable the OTel Collector two-tier architecture
  enabled: false

  # Tier 1: Router (Deployment on monitoring nodes)
  # Lightweight - routes traces by traceID to Tier 2
  router:
    image: "otel/opentelemetry-collector-contrib"
    imageTag: "0.113.0"
    imagePullPolicy: "IfNotPresent"
    # Number of router replicas - lineractl may override this
    replicas: 2
    resources:
      requests:
        cpu: "200m"
        memory: "256Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
    nodeSelector:
      workload: monitoring
    tolerations:
      - key: monitoring
        value: "true"
        effect: NoSchedule

  # Tier 2: Sampler (StatefulSet on monitoring nodes)
  # Heavy - holds traces in memory for tail sampling decisions
  sampler:
    image: "otel/opentelemetry-collector-contrib"
    imageTag: "0.113.0"
    imagePullPolicy: "IfNotPresent"
    # Replicas: scaled by lineractl based on total shard vCPUs
    # Formula: max(2, ceil(total_shard_vcpus / 50))
    replicas: 2
    resources:
      requests:
        cpu: "2000m"
        memory: "4Gi"
      limits:
        cpu: "4000m"
        memory: "6Gi"
    # Tail sampling settings
    decisionWait: "5s"
    numTraces: 100000
    expectedTracesPerSec: 5000
    latencyThresholdMs: 500
    baselineSamplingPercent: 5.0
    nodeSelector:
      workload: monitoring
    tolerations:
      - key: monitoring
        value: "true"
        effect: NoSchedule

  # Tempo endpoint (where Tier 2 exports to)
  tempoEndpoint: "tempo.tempo.svc.cluster.local:4317"
  tempoInsecure: true

# ============================================================================
# NodeLocal DNSCache - Reduces CoreDNS load by caching DNS at node level
# ============================================================================
# Deploys a DaemonSet that runs a DNS cache on every node, reducing
# latency and load on the cluster's CoreDNS pods.

nodeLocalDns:
  # Enable/disable NodeLocal DNSCache
  enabled: false

  # Local DNS IP - link-local address for the node cache
  # Must not conflict with any existing cluster IPs
  localDnsIp: "169.254.20.10"

  # kube-dns service IP - REQUIRED for iptables interception
  # Get this with: kubectl get svc kube-dns -n kube-system -o jsonpath='{.spec.clusterIP}'
  # For GKE, typically 10.x.0.10 where x depends on your service CIDR
  kubeDnsIp: ""

  # kube-dns-upstream service IP - Used for forwarding DNS queries
  # This service bypasses NOTRACK iptables rules, allowing NAT to work properly
  # If empty, Helm will look up the service IP automatically (requires service to exist)
  # On first deploy, falls back to kubeDnsIp (requires second helm upgrade to fix)
  kubeDnsUpstreamIp: ""

  # Image configuration
  image: "registry.k8s.io/dns/k8s-dns-node-cache"
  imageTag: "1.23.1"
  imagePullPolicy: "IfNotPresent"

  # Resource limits - lineractl calculates proportional values based on VM vCPUs
  # Shard nodes have high DNS load from ScyllaDB connection queries
  resources:
    requests:
      cpu: "400m"
      memory: "64Mi"
    limits:
      cpu: "1200m"
      memory: "256Mi"

  # DNS configuration
  # Cluster DNS domain (usually cluster.local)
  clusterDomain: "cluster.local"

  # Upstream DNS servers for external queries (empty = use node's resolv.conf)
  upstreamServers: ""

  # Cache TTL settings (in seconds)
  # How long to cache successful DNS responses
  successTtl: 30
  # How long to cache negative DNS responses (NXDOMAIN)
  denialTtl: 5
